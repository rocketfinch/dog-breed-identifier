{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05fa53a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dog breed indentifier using CNN\n",
    "# Importing the libraries\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# print(gpu_devices)\n",
    "\n",
    "# for device in gpu_devices:\n",
    "#     tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# This disables the GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != 'GPU'\n",
    "except e:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "# from tensorflow.keras.callbacks import TensorBoard\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.applications import mobilenet_v2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "# from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "692b7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = './results/'\n",
    "IMG_SIZE = 224\n",
    "\n",
    "BATCH_SIZE_PRE = 32\n",
    "BATCH_SIZE_FINE = 32\n",
    "EPOCHS_PRE = 5\n",
    "EPOCHS_FINE = 12\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b48c716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image4d_from_file(filename:str):\n",
    "    image = cv2.imread(filename, cv2.IMREAD_COLOR).astype('float32')\n",
    "    print(\"image type:\", type(image))\n",
    "    image_4d = np.expand_dims(image, axis=0)\n",
    "    print(\"image_4d:\", image_4d.shape)\n",
    "    return image_4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64b67fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heavily inspired by Xception-with-Your-Own-Dataset implementation of \n",
    "def generator_from_filenames_and_labels(filenames, labels, batch_size, input_size=(299,299)):\n",
    "    num_files = len(filenames)\n",
    "    while 1:\n",
    "        permutation = np.random.permutation(num_files)\n",
    "        inputs_shuffled = filenames[permutation]\n",
    "        labels_shuffled = labels[permutation]\n",
    "        for i in range(0, numsamples, batch_size):\n",
    "            inputs = list(map(lambda x: image.load_img(x, target_size=input_size), filenames[i:i+batch_size]))\n",
    "            inputs = np.array(list(map(lambda x: image.img_to_array(x))))\n",
    "            inputs = pre_process_inputs(inputs)\n",
    "            yield (inputs, labels[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7096ab26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18680 images belonging to 172 classes.\n",
      "Found 18680 images belonging to 172 classes.\n",
      "Found 9355 images belonging to 172 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True,\n",
    "    rotation_range = 10,\n",
    "    horizontal_flip = True,\n",
    "    validation_split = 0.2)\n",
    "\n",
    "train_pre_generator = train_datagen.flow_from_directory(\n",
    "    directory = \"./data/fusion/train/\",\n",
    "    target_size = (IMG_SIZE, IMG_SIZE),\n",
    "    color_mode = \"rgb\",\n",
    "    batch_size = BATCH_SIZE_PRE,\n",
    "    class_mode = \"categorical\")\n",
    "\n",
    "train_fine_generator = train_datagen.flow_from_directory(\n",
    "    directory = \"./data/fusion/train/\",\n",
    "    target_size = (IMG_SIZE, IMG_SIZE),\n",
    "    color_mode = \"rgb\",\n",
    "    batch_size = BATCH_SIZE_FINE,\n",
    "    class_mode = \"categorical\")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255\n",
    ")\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory = \"./data/fusion/test/\",\n",
    "    batch_size = BATCH_SIZE_FINE,\n",
    "    class_mode = \"categorical\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5c62dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importing the dataset\n",
    "# dog_breed_name = pd.read_csv('./data/labels_original.csv')\n",
    "# dog_breed_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3620d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find files in the directory\n",
    "# images_train = os.listdir(\"./data/fusion/train/\")\n",
    "# # import test data set\n",
    "# image_test = os.listdir(\"./data/fusion/test/\")\n",
    "\n",
    "# # from https://stackoverflow.com/questions/2632205/how-to-count-the-number-of-files-in-a-directory-using-python\n",
    "# num_images_train = len([name for name in os.listdir('./data/fusion/train/') if os.path.isfile(name)])\n",
    "# num_images_test = len([name for name in os.listdir('./data/fusion/test/') if os.path.isfile(name)])\n",
    "\n",
    "# print(num_images_train)\n",
    "# print(num_images_test)\n",
    "# print(len(images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba8645c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create target variable\n",
    "# dog_breed_name['target'] = dog_breed_name['breed'].astype('category').cat.codes\n",
    "# dog_breed_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4ad9474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_breeds: 172\n"
     ]
    }
   ],
   "source": [
    "# dog_breed_dict = {}\n",
    "# for i, breed in enumerate(dog_breed_name['breed'].unique()):\n",
    "#     dog_breed_dict[breed] = i\n",
    "    \n",
    "# num_breeds = len(dog_breed_name['breed'].unique())\n",
    "# print(\"num_breeds:\", num_breeds)\n",
    "\n",
    "assert len(os.listdir('./data/fusion/train/')) == len(os.listdir('./data/fusion/test/'))\n",
    "num_breeds = len(os.listdir('./data/fusion/train/')) - 1\n",
    "print(\"num_breeds:\", num_breeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fe43995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 00:28:33.501845: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "mobilenetv2_base_model = mobilenet_v2.MobileNetV2(include_top=False,\n",
    "                                        weights='imagenet',\n",
    "                                        input_shape=(IMG_SIZE,IMG_SIZE,3))\n",
    "\n",
    "x_base_mobilenetv2 = mobilenetv2_base_model.output\n",
    "x_avg_pooling_mobilenetv2 = GlobalAveragePooling2D()(x_base_mobilenetv2)\n",
    "x_first_fc_mobilenetv2 = Dense(1024, activation='relu')(x_avg_pooling_mobilenetv2)\n",
    "x_last_fc_mobilenetv2 = Dense(num_breeds, activation='softmax')(x_first_fc_mobilenetv2)\n",
    "\n",
    "mobilenetv2_custom_model = Model(inputs=mobilenetv2_base_model.inputs, outputs=x_last_fc_mobilenetv2)\n",
    "\n",
    "# xception_custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78a27eb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104902/2026603185.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_train_pre = mobilenetv2_custom_model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "583/583 [==============================] - 253s 432ms/step - loss: 0.6016 - accuracy: 0.8108 - val_loss: 1.5169 - val_accuracy: 0.5879\n",
      "Epoch 2/5\n",
      "583/583 [==============================] - 245s 420ms/step - loss: 0.4843 - accuracy: 0.8453 - val_loss: 1.5118 - val_accuracy: 0.5954\n",
      "Epoch 3/5\n",
      "583/583 [==============================] - 249s 427ms/step - loss: 0.4296 - accuracy: 0.8636 - val_loss: 1.5063 - val_accuracy: 0.5926\n",
      "Epoch 4/5\n",
      "583/583 [==============================] - 250s 428ms/step - loss: 0.3974 - accuracy: 0.8726 - val_loss: 1.5267 - val_accuracy: 0.5917\n",
      "Epoch 5/5\n",
      "583/583 [==============================] - 251s 431ms/step - loss: 0.3609 - accuracy: 0.8829 - val_loss: 1.5543 - val_accuracy: 0.5890\n"
     ]
    }
   ],
   "source": [
    "#Train model - freeze body layers first\n",
    "for layer in mobilenetv2_base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "mobilenetv2_custom_model.compile(optimizer=SGD(momentum=MOMENTUM), loss=categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "model_train_pre = mobilenetv2_custom_model.fit_generator(\n",
    "    generator = train_pre_generator,\n",
    "    steps_per_epoch = (train_pre_generator.n//train_pre_generator.batch_size),\n",
    "    epochs = EPOCHS_PRE,\n",
    "    validation_data = test_generator,\n",
    "    validation_steps = (test_generator.n//test_generator.batch_size),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "mobilenetv2_custom_model.save(os.path.join(RESULTS_DIR, 'model_pre_mbnet2_3.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d611495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104902/428998347.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_train_fine = mobilenetv2_custom_model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 701s 599ms/step - loss: 2.5809 - accuracy: 0.3339 - val_loss: 12.4583 - val_accuracy: 0.0079\n",
      "Epoch 2/12\n",
      "1167/1167 [==============================] - 707s 606ms/step - loss: 1.9728 - accuracy: 0.4546 - val_loss: 6.5063 - val_accuracy: 0.0691\n",
      "Epoch 3/12\n",
      "1167/1167 [==============================] - 719s 616ms/step - loss: 1.6384 - accuracy: 0.5299 - val_loss: 3.9617 - val_accuracy: 0.1809\n",
      "Epoch 4/12\n",
      "1167/1167 [==============================] - 716s 614ms/step - loss: 1.4372 - accuracy: 0.5803 - val_loss: 5.4386 - val_accuracy: 0.0971\n",
      "Epoch 5/12\n",
      "1167/1167 [==============================] - 714s 612ms/step - loss: 1.2800 - accuracy: 0.6191 - val_loss: 2.8422 - val_accuracy: 0.3264\n",
      "Epoch 6/12\n",
      "1167/1167 [==============================] - 714s 612ms/step - loss: 1.1402 - accuracy: 0.6502 - val_loss: 3.3078 - val_accuracy: 0.2501\n",
      "Epoch 7/12\n",
      "1167/1167 [==============================] - 712s 610ms/step - loss: 1.0361 - accuracy: 0.6813 - val_loss: 2.3004 - val_accuracy: 0.4152\n",
      "Epoch 8/12\n",
      "1167/1167 [==============================] - 712s 610ms/step - loss: 0.9424 - accuracy: 0.7082 - val_loss: 2.7874 - val_accuracy: 0.3273\n",
      "Epoch 9/12\n",
      "1167/1167 [==============================] - 714s 612ms/step - loss: 0.8523 - accuracy: 0.7337 - val_loss: 2.1736 - val_accuracy: 0.4414\n",
      "Epoch 10/12\n",
      "1167/1167 [==============================] - 710s 609ms/step - loss: 0.7910 - accuracy: 0.7513 - val_loss: 2.5695 - val_accuracy: 0.3988\n",
      "Epoch 11/12\n",
      "1167/1167 [==============================] - 712s 611ms/step - loss: 0.7014 - accuracy: 0.7735 - val_loss: 1.9259 - val_accuracy: 0.5214\n",
      "Epoch 12/12\n",
      "1167/1167 [==============================] - 714s 612ms/step - loss: 0.6448 - accuracy: 0.7908 - val_loss: 2.1557 - val_accuracy: 0.4732\n"
     ]
    }
   ],
   "source": [
    "#Train model - train all layers\n",
    "for layer in mobilenetv2_base_model.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "mobilenetv2_custom_model.compile(optimizer=SGD(momentum=MOMENTUM), loss=categorical_crossentropy, metrics=['accuracy'] )\n",
    "\n",
    "model_train_fine = mobilenetv2_custom_model.fit_generator(\n",
    "    generator = train_fine_generator,\n",
    "    steps_per_epoch = (train_fine_generator.n//train_fine_generator.batch_size),\n",
    "    epochs = EPOCHS_FINE,\n",
    "    validation_data = test_generator,\n",
    "    validation_steps = (test_generator.n//test_generator.batch_size),\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "mobilenetv2_custom_model.save(os.path.join(RESULTS_DIR, 'model_fine_mbnet2.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7141cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def image_predict_from_model(filename:str):\n",
    "#     image_4d = read_image_4d(filename)\n",
    "#     preprocessed_image = preprocess_input(raw_image)\n",
    "#     prediction_probs = xception_model.predict_generator(preprocessed_image)\n",
    "#     best_prediction = np.argmax(prediction_probs)\n",
    "    \n",
    "# #     features = xception_models.predict(image)\n",
    "# #     return features.reshape(1, 7*7*512)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61222ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109829/2465302789.py:3: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  y_pred = mobilenetv2_custom_model.predict_generator(test_generator, steps=test_generator.samples // BATCH_SIZE_FINE, verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292/292 [==============================] - 86s 293ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'f1_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_pred, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m y_true \u001b[38;5;241m=\u001b[39m test_generator\u001b[38;5;241m.\u001b[39mclasses\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1 score:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mf1_score\u001b[49m(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f1_score' is not defined"
     ]
    }
   ],
   "source": [
    "# From Ananya, f-1 score\n",
    "mobilenetv2_custom_model= load_model(os.path.join(RESULTS_DIR, 'model_pre_mbnet2_2.h5'))\n",
    "y_pred = mobilenetv2_custom_model.predict_generator(test_generator, steps=test_generator.samples // BATCH_SIZE_FINE, verbose=1)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "y_true = test_generator.classes\n",
    "\n",
    "print('f1 score:', f1_score(y_true, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bbf826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
