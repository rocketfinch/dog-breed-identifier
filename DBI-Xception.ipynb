{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05fa53a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 01:31:58.613084: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-13 01:31:59.089029: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/corey/miniconda3/envs/tf/lib/:/home/corey/miniconda3/envs/tf/lib/\n",
      "2022-12-13 01:31:59.089070: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/corey/miniconda3/envs/tf/lib/:/home/corey/miniconda3/envs/tf/lib/\n",
      "2022-12-13 01:31:59.089074: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-13 01:31:59.592161: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-12-13 01:31:59.592182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: cally\n",
      "2022-12-13 01:31:59.592186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: cally\n",
      "2022-12-13 01:31:59.592240: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 525.60.11\n",
      "2022-12-13 01:31:59.592253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 525.60.11\n",
      "2022-12-13 01:31:59.592256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 525.60.11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#dog breed indentifier using CNN\n",
    "# Importing the libraries\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# print(gpu_devices)\n",
    "\n",
    "# for device in gpu_devices:\n",
    "#     tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# This disables the GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != 'GPU'\n",
    "except e:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "# from tensorflow.keras.callbacks import TensorBoard\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.applications import xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "# from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "692b7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = './results/'\n",
    "IMG_SIZE = 299\n",
    "\n",
    "BATCH_SIZE_PRE = 32\n",
    "BATCH_SIZE_FINE = 16\n",
    "EPOCHS_PRE = 10\n",
    "EPOCHS_FINE = 20\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b48c716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image4d_from_file(filename:str):\n",
    "    image = cv2.imread(filename, cv2.IMREAD_COLOR).astype('float32')\n",
    "    print(\"image type:\", type(image))\n",
    "    image_4d = np.expand_dims(image, axis=0)\n",
    "    print(\"image_4d:\", image_4d.shape)\n",
    "    return image_4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b67fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heavily inspired by Xception-with-Your-Own-Dataset implementation of \n",
    "def generator_from_filenames_and_labels(filenames, labels, batch_size, input_size=(299,299)):\n",
    "    num_files = len(filenames)\n",
    "    while 1:\n",
    "        permutation = np.random.permutation(num_files)\n",
    "        inputs_shuffled = filenames[permutation]\n",
    "        labels_shuffled = labels[permutation]\n",
    "        for i in range(0, numsamples, batch_size):\n",
    "            inputs = list(map(lambda x: image.load_img(x, target_size=input_size), filenames[i:i+batch_size]))\n",
    "            inputs = np.array(list(map(lambda x: image.img_to_array(x))))\n",
    "            inputs = pre_process_inputs(inputs)\n",
    "            yield (inputs, labels[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7096ab26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18680 images belonging to 172 classes.\n",
      "Found 18680 images belonging to 172 classes.\n",
      "Found 9355 images belonging to 172 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True,\n",
    "    rotation_range = 10,\n",
    "    horizontal_flip = True,\n",
    "    validation_split = 0.2)\n",
    "\n",
    "train_pre_generator = train_datagen.flow_from_directory(\n",
    "    directory = \"./data/fusion/train/\",\n",
    "    target_size = (IMG_SIZE, IMG_SIZE),\n",
    "    color_mode = \"rgb\",\n",
    "    batch_size = BATCH_SIZE_PRE,\n",
    "    class_mode = \"categorical\")\n",
    "\n",
    "train_fine_generator = train_datagen.flow_from_directory(\n",
    "    directory = \"./data/fusion/train/\",\n",
    "    target_size = (IMG_SIZE, IMG_SIZE),\n",
    "    color_mode = \"rgb\",\n",
    "    batch_size = BATCH_SIZE_FINE,\n",
    "    class_mode = \"categorical\")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255\n",
    ")\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory = \"./data/fusion/test/\",\n",
    "    batch_size = BATCH_SIZE_FINE,\n",
    "    class_mode = \"categorical\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c62dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importing the dataset\n",
    "# dog_breed_name = pd.read_csv('./data/labels_original.csv')\n",
    "# dog_breed_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3620d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find files in the directory\n",
    "# images_train = os.listdir(\"./data/fusion/train/\")\n",
    "# # import test data set\n",
    "# image_test = os.listdir(\"./data/fusion/test/\")\n",
    "\n",
    "# # from https://stackoverflow.com/questions/2632205/how-to-count-the-number-of-files-in-a-directory-using-python\n",
    "# num_images_train = len([name for name in os.listdir('./data/fusion/train/') if os.path.isfile(name)])\n",
    "# num_images_test = len([name for name in os.listdir('./data/fusion/test/') if os.path.isfile(name)])\n",
    "\n",
    "# print(num_images_train)\n",
    "# print(num_images_test)\n",
    "# print(len(images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba8645c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create target variable\n",
    "# dog_breed_name['target'] = dog_breed_name['breed'].astype('category').cat.codes\n",
    "# dog_breed_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4ad9474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_breeds: 172\n"
     ]
    }
   ],
   "source": [
    "# dog_breed_dict = {}\n",
    "# for i, breed in enumerate(dog_breed_name['breed'].unique()):\n",
    "#     dog_breed_dict[breed] = i\n",
    "    \n",
    "# num_breeds = len(dog_breed_name['breed'].unique())\n",
    "# print(\"num_breeds:\", num_breeds)\n",
    "\n",
    "assert len(os.listdir('./data/fusion/train/')) == len(os.listdir('./data/fusion/test/'))\n",
    "num_breeds = len(os.listdir('./data/fusion/train/')) - 1\n",
    "print(\"num_breeds:\", num_breeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fe43995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 01:32:00.727932: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "xception_base_model = xception.Xception(include_top=False,\n",
    "                                        weights='imagenet',\n",
    "                                        input_shape=(299,299,3))\n",
    "\n",
    "x_base_xception = xception_base_model.output\n",
    "x_avg_pooling_xception = GlobalAveragePooling2D()(x_base_xception)\n",
    "x_first_fc_xception = Dense(1024, activation='relu')(x_avg_pooling_xception)\n",
    "x_last_fc_xception = Dense(num_breeds, activation='softmax')(x_first_fc_xception)\n",
    "\n",
    "xception_custom_model = Model(inputs=xception_base_model.inputs, outputs=x_last_fc_xception)\n",
    "\n",
    "# xception_custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78a27eb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_113337/1785464405.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_train_pre = xception_custom_model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "583/583 [==============================] - 1170s 2s/step - loss: 1.6356 - accuracy: 0.6854 - val_loss: 0.7242 - val_accuracy: 0.7940\n",
      "Epoch 2/10\n",
      "583/583 [==============================] - 1138s 2s/step - loss: 0.5562 - accuracy: 0.8327 - val_loss: 0.6964 - val_accuracy: 0.8064\n",
      "Epoch 3/10\n",
      "583/583 [==============================] - 1142s 2s/step - loss: 0.4682 - accuracy: 0.8511 - val_loss: 0.7026 - val_accuracy: 0.8105\n",
      "Epoch 4/10\n",
      "583/583 [==============================] - 1136s 2s/step - loss: 0.4182 - accuracy: 0.8671 - val_loss: 0.6832 - val_accuracy: 0.8176\n",
      "Epoch 5/10\n",
      "583/583 [==============================] - 1133s 2s/step - loss: 0.3843 - accuracy: 0.8736 - val_loss: 0.7260 - val_accuracy: 0.8098\n",
      "Epoch 6/10\n",
      "583/583 [==============================] - 1141s 2s/step - loss: 0.3568 - accuracy: 0.8828 - val_loss: 0.7153 - val_accuracy: 0.8161\n",
      "Epoch 7/10\n",
      "583/583 [==============================] - 1136s 2s/step - loss: 0.3288 - accuracy: 0.8901 - val_loss: 0.7299 - val_accuracy: 0.8175\n",
      "Epoch 8/10\n",
      "583/583 [==============================] - 1134s 2s/step - loss: 0.3088 - accuracy: 0.8975 - val_loss: 0.7169 - val_accuracy: 0.8189\n",
      "Epoch 9/10\n",
      "583/583 [==============================] - 1125s 2s/step - loss: 0.2912 - accuracy: 0.9029 - val_loss: 0.7376 - val_accuracy: 0.8169\n",
      "Epoch 10/10\n",
      "583/583 [==============================] - 1134s 2s/step - loss: 0.2760 - accuracy: 0.9082 - val_loss: 0.7623 - val_accuracy: 0.8128\n"
     ]
    }
   ],
   "source": [
    "#Train model - freeze body layers first\n",
    "for layer in xception_base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "xception_custom_model.compile(optimizer=SGD(momentum=MOMENTUM), loss=categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "model_train_pre = xception_custom_model.fit_generator(\n",
    "    generator = train_pre_generator,\n",
    "    steps_per_epoch = (train_pre_generator.n//train_pre_generator.batch_size),\n",
    "    epochs = EPOCHS_PRE,\n",
    "    validation_data = test_generator,\n",
    "    validation_steps = (test_generator.n//test_generator.batch_size),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "xception_custom_model.save(os.path.join(RESULTS_DIR, 'model_pre_2.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d611495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_110808/3989455278.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_train_fine = xception_custom_model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   2/1167 [..............................] - ETA: 1:13:05 - loss: 5.1508 - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "#Train model - train all layers\n",
    "for layer in xception_base_model.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "xception_custom_model.compile(optimizer=SGD(momentum=0.9), loss=categorical_crossentropy, metrics=['accuracy'] )\n",
    "\n",
    "model_train_fine = xception_custom_model.fit_generator(\n",
    "    generator = train_fine_generator,\n",
    "    steps_per_epoch = (train_fine_generator.n//train_fine_generator.batch_size),\n",
    "    epochs = EPOCHS_FINE,\n",
    "    validation_data = test_generator,\n",
    "    validation_steps = (test_generator.n//test_generator.batch_size),\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "xception_custom_model.save(os.path.join(RESULTS_DIR, 'model_fine.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7141cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def image_predict_from_model(filename:str):\n",
    "#     image_4d = read_image_4d(filename)\n",
    "#     preprocessed_image = preprocess_input(raw_image)\n",
    "#     prediction_probs = xception_model.predict_generator(preprocessed_image)\n",
    "#     best_prediction = np.argmax(prediction_probs)\n",
    "    \n",
    "# #     features = xception_models.predict(image)\n",
    "# #     return features.reshape(1, 7*7*512)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bea669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f-1 score\n",
    "xception_custom_model= load_model(os.path.join(RESULTS_DIR, 'model_pre.h5'))\n",
    "y_pred = xception_custom_model.predict_generator(test_generator, steps=test_generator.samples // BATCH_SIZE_FINE, verbose=1)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "y_true = test_generator.classes\n",
    "\n",
    "print('f1 score:', f1_score(y_true, y_pred, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
